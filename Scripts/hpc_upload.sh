#!/bin/bash

# Simple script to upload and submit the QIIME pipeline to an HPC cluster via Slurm
# Usage: bash hpc_upload.sh [HPC_HOST] [HPC_PROJECT_DIR]
# Example: bash hpc_upload.sh user@hpc.university.edu /scratch/user/QIIME

set -euo pipefail

# Configuration
HPC_HOST="${1:-}"
HPC_PROJECT_DIR="${2:-}"
LOCAL_PROJECT_DIR="$(cd "$(dirname "$0")/.." && pwd)"

# Help message
if [[ -z "${HPC_HOST}" ]] || [[ "${HPC_HOST}" == "-h" ]] || [[ "${HPC_HOST}" == "--help" ]]; then
    cat << 'EOF'
Usage: bash hpc_upload.sh HPC_HOST HPC_PROJECT_DIR

Upload QIIME pipeline to HPC cluster and submit as Slurm job.

Arguments:
  HPC_HOST          SSH connection string (e.g., user@hpc.university.edu)
  HPC_PROJECT_DIR   Remote project directory on HPC (e.g., /scratch/user/QIIME)

Example:
  bash hpc_upload.sh user@hpc.edu /scratch/user/QIIME

What this script does:
  1. Syncs local project directory to HPC using rsync
  2. Creates a temporary sbatch script with your settings
  3. Submits the job to Slurm queue
  4. Shows job status

Before running:
  - Set up SSH key authentication to HPC
  - Verify HPC_PROJECT_DIR exists or will be created
  - Customize pipeline parameters below if needed

EOF
    exit 0
fi

if [[ -z "${HPC_PROJECT_DIR}" ]]; then
    echo "ERROR: Both HPC_HOST and HPC_PROJECT_DIR are required"
    echo "Run with --help for usage information"
    exit 1
fi

echo "=== QIIME Pipeline HPC Upload & Submit ==="
echo "Local project:  ${LOCAL_PROJECT_DIR}"
echo "HPC host:       ${HPC_HOST}"
echo "HPC directory:  ${HPC_PROJECT_DIR}"
echo ""

# Test SSH connection
echo "Testing SSH connection..."
if ! ssh -o ConnectTimeout=10 "${HPC_HOST}" "echo 'Connection successful'" 2>/dev/null; then
    echo "ERROR: Cannot connect to ${HPC_HOST}"
    echo "Please check:"
    echo "  1. SSH key is set up (ssh-copy-id ${HPC_HOST})"
    echo "  2. Host is correct and accessible"
    exit 1
fi
echo "✓ SSH connection successful"
echo ""

# Create remote directory if needed
echo "Creating remote directory..."
ssh "${HPC_HOST}" "mkdir -p ${HPC_PROJECT_DIR}"
echo "✓ Remote directory ready"
echo ""

# Sync project files to HPC
echo "Syncing project files to HPC..."
rsync -avz --progress \
    --exclude '.git' \
    --exclude '__pycache__' \
    --exclude '*.pyc' \
    --exclude '.DS_Store' \
    --exclude 'Results/' \
    --exclude 'Data/processed_data/' \
    --exclude 'Logs/' \
    "${LOCAL_PROJECT_DIR}/" \
    "${HPC_HOST}:${HPC_PROJECT_DIR}/"
echo "✓ Files synced"
echo ""

# Pipeline parameters (customize these)
PIPELINE_PARAMS=$(cat << 'PARAMS'
# Pipeline configuration - edit these values as needed
export START_STEP=1
export MODE=denoise
export ENV_NAME=qiime
export N_THREADS=8
export TRUNC_LEN_F=240
export TRUNC_LEN_R=200
# export DECONTAMINATION_CHOICE=1
# export DECONTAM_THRESHOLD=0.5
# export CLASSIFIER_PATH=Data/reference_dbs/classifier.qza
PARAMS
)

# Create sbatch script on HPC
echo "Creating sbatch job script on HPC..."
ssh "${HPC_HOST}" "cat > ${HPC_PROJECT_DIR}/submit_pipeline.sbatch" << EOF
#!/bin/bash
#SBATCH --job-name=qiime_pipeline
#SBATCH --output=${HPC_PROJECT_DIR}/Logs/slurm-pipeline-%j.out
#SBATCH --error=${HPC_PROJECT_DIR}/Logs/slurm-pipeline-%j.err
#SBATCH --time=24:00:00
#SBATCH --cpus-per-task=8
#SBATCH --mem=64G
#SBATCH --partition=standard

# Auto-generated sbatch script for QIIME pipeline
# Generated by hpc_upload.sh on $(date)

set -euo pipefail

# Change to project directory
cd ${HPC_PROJECT_DIR}

# Create Logs directory
mkdir -p Logs

# Load conda (customize for your HPC)
source "\$(conda info --base)/etc/profile.d/conda.sh" || module load anaconda

# Activate QIIME environment
conda activate qiime

# Set pipeline parameters
${PIPELINE_PARAMS}

# Run pipeline in non-interactive mode
bash Scripts/main.sh --non-interactive

echo "Pipeline completed at \$(date)"
EOF

echo "✓ sbatch script created"
echo ""

# Submit job to Slurm
echo "Submitting job to Slurm queue..."
JOB_ID=$(ssh "${HPC_HOST}" "cd ${HPC_PROJECT_DIR} && sbatch submit_pipeline.sbatch" | grep -oP '\d+')

if [[ -n "${JOB_ID}" ]]; then
    echo "✓ Job submitted successfully!"
    echo "  Job ID: ${JOB_ID}"
    echo ""
    echo "Monitor job status with:"
    echo "  ssh ${HPC_HOST} 'squeue -j ${JOB_ID}'"
    echo ""
    echo "View job output:"
    echo "  ssh ${HPC_HOST} 'tail -f ${HPC_PROJECT_DIR}/Logs/slurm-pipeline-${JOB_ID}.out'"
    echo ""
    echo "Cancel job if needed:"
    echo "  ssh ${HPC_HOST} 'scancel ${JOB_ID}'"
else
    echo "ERROR: Job submission failed"
    exit 1
fi
